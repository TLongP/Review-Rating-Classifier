{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_tfrecord, list_files_in_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_tfrecord(list_files_in_path(\"datasets\\\\tfrecords\\\\train\"))\n",
    "valid_set = load_tfrecord(list_files_in_path(\"datasets\\\\tfrecords\\\\validation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"taking the parameters from the first model see model_tuning.ipynb \"\"\"\n",
    "num_of_words = 10000\n",
    "max_sentence_len = 200\n",
    "embedding_dim = 16\n",
    "lstm_1_dim = 32\n",
    "lstm_2_dim = 32\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorize_dict = {\n",
    "    \"max_tokens\": num_of_words,\n",
    "    \"output_mode\": \"int\",\n",
    "    \"output_sequence_length\":max_sentence_len\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(**text_vectorize_dict)\n",
    "vectorize_layer._name=\"Text_Vectorization_Layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(train_set.map(lambda x,y: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"this class is also used in transformer\n",
    "        so for each position in sentence this class add a well defined position vector\n",
    "    \"\"\"\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "postional_encoding = PositionalEncoding(max_sentence_len,embedding_dim)\n",
    "embedding_layer = layers.Embedding(input_dim=num_of_words,output_dim=embedding_dim,input_length=max_sentence_len)\n",
    "lstm_layer_1 = layers.LSTM(lstm_1_dim,return_sequences=True)\n",
    "lstm_layer_2 = layers.LSTM(lstm_2_dim)\n",
    "output_layer = layers.Dense(1,activation=\"sigmoid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"in short our model is the 'encoder' in the transformer model \"\"\"\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    postional_encoding,\n",
    "    lstm_layer_1,\n",
    "    lstm_layer_2,\n",
    "    output_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Text_Vectorization_Layer (T  (None, 200)              0         \n",
      " extVectorization)                                               \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 16)           160000    \n",
      "                                                                 \n",
      " positional_encoding (Positi  (None, 200, 16)          0         \n",
      " onalEncoding)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 32)           6272      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174,625\n",
      "Trainable params: 174,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"BinaryCrossentropy\",metrics=['accuracy'],optimizer=Adam(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 77s 58ms/step - loss: 0.6761 - accuracy: 0.5462 - val_loss: 0.5072 - val_accuracy: 0.7740\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 53s 43ms/step - loss: 0.4188 - accuracy: 0.8173 - val_loss: 0.3573 - val_accuracy: 0.8466\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.3200 - accuracy: 0.8665 - val_loss: 0.3389 - val_accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 59s 47ms/step - loss: 0.2633 - accuracy: 0.8942 - val_loss: 0.3526 - val_accuracy: 0.8668\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.2145 - accuracy: 0.9187 - val_loss: 0.3498 - val_accuracy: 0.8682\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 63s 50ms/step - loss: 0.1855 - accuracy: 0.9326 - val_loss: 0.3559 - val_accuracy: 0.8640\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 57s 45ms/step - loss: 0.1617 - accuracy: 0.9425 - val_loss: 0.3987 - val_accuracy: 0.8580\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.1397 - accuracy: 0.9522 - val_loss: 0.4265 - val_accuracy: 0.8530\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 53s 43ms/step - loss: 0.1313 - accuracy: 0.9533 - val_loss: 0.4397 - val_accuracy: 0.8570\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 59s 47ms/step - loss: 0.1360 - accuracy: 0.9521 - val_loss: 0.4293 - val_accuracy: 0.8542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aae6c6d880>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"le5t first test on 10 epochs\"\"\"\n",
    "model.fit(train_set,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Text_Vectorization_Layer (T  (None, 200)              0         \n",
      " extVectorization)                                               \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 16)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 32)           6272      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174,625\n",
      "Trainable params: 174,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for comparison we will try on a model without positional_encoding\"\"\"\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    #postional_encoding, taking out this layer\n",
    "    lstm_layer_1,\n",
    "    lstm_layer_2,\n",
    "    output_layer\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"BinaryCrossentropy\",metrics=['accuracy'],optimizer=Adam(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 68s 53ms/step - loss: 0.1472 - accuracy: 0.9475 - val_loss: 0.4504 - val_accuracy: 0.8566\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 56s 45ms/step - loss: 0.1254 - accuracy: 0.9561 - val_loss: 0.4454 - val_accuracy: 0.8522\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 54s 43ms/step - loss: 0.1213 - accuracy: 0.9568 - val_loss: 0.4619 - val_accuracy: 0.8524\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 57s 45ms/step - loss: 0.0965 - accuracy: 0.9673 - val_loss: 0.5043 - val_accuracy: 0.8596\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0766 - accuracy: 0.9757 - val_loss: 0.5216 - val_accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 75s 60ms/step - loss: 0.0718 - accuracy: 0.9765 - val_loss: 0.5470 - val_accuracy: 0.8524\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 78s 63ms/step - loss: 0.0684 - accuracy: 0.9780 - val_loss: 0.5975 - val_accuracy: 0.8498\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 71s 56ms/step - loss: 0.0635 - accuracy: 0.9800 - val_loss: 0.5858 - val_accuracy: 0.8528\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 59s 47ms/step - loss: 0.0607 - accuracy: 0.9804 - val_loss: 0.5949 - val_accuracy: 0.8456\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 55s 44ms/step - loss: 0.0565 - accuracy: 0.9823 - val_loss: 0.6533 - val_accuracy: 0.8494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab43ea6490>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"it does not seem to be better!\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1a276f21c0508512e6722d264d598eca25136e3830f5054f95dc6663d74932f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
